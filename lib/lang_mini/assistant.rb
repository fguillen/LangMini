# frozen_string_literal: true

module LangMini

  # Represents an Assistant that helps interacting with the model
  # An Assistant relates to a `model`, and the `client` that holds
  # the `complete` method.
  #
  # @example Assistant first message
  #   client = LangMini::Client.new(access_token: ENV["OPEN_ROUTER_KEY"])
  #   assistant =
  #     LangMini::Assistant.new(
  #       client: client,
  #       model: "meta-llama/llama-3.1-8b-instruct:free"
  #     )
  #
  #   message = LangMini::Message.from_hash({ role: "user", content: "What is the color of the sky?" })
  #   new_messages = assistant.completion(message:)
  #
  #   new_messages[0].data[:content] # => "What is the color of the sky?"
  #   new_messages[1].data[:content] # => "The color of the sky appears to change ..."
  class Assistant
    attr_reader :conversation
    attr_reader :model

    # Creates a new instance of Assistant
    #
    # @param client [LangMini::Client] The client that is making the API request.
    #   This client is responsible for the actual API call.
    # @param model [String] The name of the model from the [OpenRouter available](https://openrouter.ai/models)
    #   to be used.
    # @param conversation [LangMini::Conversation] In case the conversation has
    #   started it can receive the previous messages.
    # @param on_new_message [Block] Callback to be invoked each time a new
    #   message is generated.
    # @return [LangMini::Assistant]
    def initialize(
      client:,
      model: "openrouter/auto",
      conversation: nil,
      on_new_message: nil
    )
      @client = client
      @system_directive = system_directive
      @model = model
      @conversation = conversation || LangMini::Conversation.new
      @on_new_message = on_new_message

      # After initializing the Assistant, this method is called to do extra setup.
      after_initialize
    end

    # Called after initializing the Assistant. This method can be overridden
    # to do extra setup.
    def after_initialize
    end

    # Request a completion from the model. A completion is basically an 'answer' from the model.
    #
    # @param message [LangMini::Message]
    # @param model [String] The name of the model from the [OpenRouter available](https://openrouter.ai/models)
    #   to be used.
    #
    # @return [Array<LangMini::Message>] The new messages that were generated by the model.
    def completion(message:, model: @model)
      init_system_directive

      add_new_message(message)

      complete

      @conversation.new_messages
    end

    # The system directive is a string that is sent to the model as a message,
    # it is included in the conversation implictly. It is used to give the model
    # information about the context of the conversation. And instructions for the model
    # of how generate the completions/answers.
    #
    # This method can be overridden to define a system directive.
    #
    # @return [String, nil] The system directive string, or nil if it is not defined.
    def system_directive
      nil
    end

    # Returns an array of tools that this assistant supports.
    #
    # This method can be overridden to define the array of tools.
    #
    # @example Assistant with the tool Math
    #   class MyAssistant < LangMini::Assistant
    #     def tools
    #       [
    #         LangMini::Tools::Math.new
    #       ]
    #     end
    #   end
    #
    # @return [Array<LangMini::Tools::Tool>, nil] The tools that this assistant supports, or nil if it is not defined.
    def tools
      nil
    end

    # This method is called after the assistant has been initialized. It can be
    # overridden in sub-classes to do extra setup.
    #
    # @example Assistant with extra setup
    #   class MyAssistant < LangMini::Assistant
    #     def after_initialize
    #       # Do extra setup here
    #     end
    #   end
    #
    def after_initialize
    end

    private

    # From: https://github.com/patterns-ai-core/langchainrb/blob/ff699356068bd7d6bc768e5518f6b4fdcbdfc90f/lib/langchain/assistants/assistant.rb#L275
    def run_tools(tool_calls)
      # Iterate over each function invocation and submit tool output
      tool_calls.each do |tool_call|
        run_tool(tool_call)
      end

      complete
    end

    def run_tool(tool_call)
      tool_call_id, tool_name, method_name, tool_arguments = extract_tool_call_args(tool_call)

      log("run_tool: tool_call_id: #{tool_call_id}, tool_name: #{tool_name}, method_name: #{method_name}, tool_arguments: #{tool_arguments}")

      output = nil
      begin
        tool_instance =
          tools.find do |t|
            t.class_name_sanitized == tool_name
          end or raise ArgumentError, "Tool not found in assistant.tools '#{tool_name}'"

        output = tool_instance.send(method_name, **tool_arguments)
      rescue => e
        log("Error running tool #{tool_name} with arguments #{tool_arguments}:")
        log(e.message)
        log(e.backtrace.join("\n"))
        output = e.message
      end

      log("run_tool: output: #{output}")

      submit_tool_output(tool_call_id: tool_call_id, name: tool_call[:function][:name], output: output)
    end

    def complete
      response =
        @client.complete(
          @conversation.messages_data,
          model: @model,
          extras: {
            tools: extract_tools,
            temperature: 0
          }
        )

      completion = LangMini::Completion.new(response)
      message = LangMini::Message.from_completion(completion)
      add_new_message(message)

      if(completion.tools?)
        run_tools(completion.message[:tool_calls])
      end
    end

    def init_system_directive
      return if system_directive.blank?
      return if @conversation.messages.filter do |m|
        m.role == "system" && m.content == system_directive.strip
      end.present?

      message = LangMini::Message.from_raw({ role: "system", content: system_directive })
      add_new_message(message)
    end

    def extract_tools
      return [] if tools.nil?

      tools.map do |tool|
        tool.tool_description
      end.flatten
    end




    # // Some models might include their reasoning in content
    # "message": {
    #   "role": "assistant",
    #   "content": null,
    #   "tool_calls": [
    #     {
    #       "id": "call_9pw1qnYScqvGrCH58HWCvFH6",
    #       "type": "function",
    #       "function": {
    #         "name": "get_current_weather",
    #         "arguments": "{ \"location\": \"Boston, MA\"}"
    #       }
    #     }
    #   ]
    # },
    # From: https://github.com/patterns-ai-core/langchainrb/blob/ff699356068bd7d6bc768e5518f6b4fdcbdfc90f/lib/langchain/assistants/assistant.rb#L365C1-L379C14
    def extract_tool_call_args(tool_call)
      tool_call_id = tool_call.dig(:id)

      function_name = tool_call.dig(:function, :name)
      tool_name, method_name = function_name.split("__")

      tool_arguments = tool_call.dig(:function, :arguments)
      tool_arguments = JSON.parse(tool_arguments, symbolize_names: true)

      result = [tool_call_id, tool_name, method_name, tool_arguments]
      result
    end

    # {
    #   "role": "tool",
    #   "name": "get_current_weather",
    #   "tool_call_id": "call_9pw1qnYScqvGrCH58HWCvFH6",
    #   "content": "{\"temperature\": \"22\", \"unit\": \"celsius\", \"description\": \"Sunny\"}"
    # }
    def submit_tool_output(tool_call_id:, name:, output:)
      message = LangMini::Message.from_raw({ role: "tool", tool_call_id:, name:, content: output.to_s })
      add_new_message(message)
    end

    def add_new_message(message)
      @conversation.add_message(message)
      if @on_new_message
        @on_new_message.call(message)
      end
    end

    def log(message)
      LangMini.logger.debug("LangMini::Assistant: #{message}")
    end
  end

end
